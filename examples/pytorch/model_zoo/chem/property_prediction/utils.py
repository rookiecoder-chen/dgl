import datetime
import dgl
import numpy as np
import random
import torch

from dgl.data.chem import ConcatAtomFeaturizer, atomic_number_one_hot, atom_total_degree_one_hot, \
    atom_formal_charge_one_hot, atom_chiral_tag_one_hot, atom_total_num_H_one_hot, \
    atom_hybridization_one_hot, atom_is_aromatic_one_hot, atom_mass
from functools import partial
from sklearn.metrics import roc_auc_score

def set_random_seed(seed=0):
    """Set random seed.

    Parameters
    ----------
    seed : int
        Random seed to use
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)

class Meter(object):
    """Track and summarize model performance on a dataset for
    (multi-label) binary classification."""
    def __init__(self):
        self.mask = []
        self.y_pred = []
        self.y_true = []

    def update(self, y_pred, y_true, mask):
        """Update for the result of an iteration

        Parameters
        ----------
        y_pred : float32 tensor
            Predicted molecule labels with shape (B, T),
            B for batch size and T for the number of tasks
        y_true : float32 tensor
            Ground truth molecule labels with shape (B, T)
        mask : float32 tensor
            Mask for indicating the existence of ground
            truth labels with shape (B, T)
        """
        self.y_pred.append(y_pred.detach().cpu())
        self.y_true.append(y_true.detach().cpu())
        self.mask.append(mask.detach().cpu())

    def roc_auc_averaged_over_tasks(self):
        """Compute roc-auc score for each task and return the average.

        Returns
        -------
        float
            roc-auc score averaged over all tasks
        """
        mask = torch.cat(self.mask, dim=0)
        y_pred = torch.cat(self.y_pred, dim=0)
        y_true = torch.cat(self.y_true, dim=0)
        # Todo: support categorical classes
        # This assumes binary case only
        y_pred = torch.sigmoid(y_pred)
        n_tasks = y_true.shape[1]
        total_score = 0
        for task in range(n_tasks):
            task_w = mask[:, task]
            task_y_true = y_true[:, task][task_w != 0].numpy()
            task_y_pred = y_pred[:, task][task_w != 0].numpy()
            total_score += roc_auc_score(task_y_true, task_y_pred)
        return total_score / n_tasks

class EarlyStopping(object):
    """Early stop performing

    Parameters
    ----------
    mode : str
        * 'higher': Higher metric suggests a better model
        * 'lower': Lower metric suggests a better model
    patience : int
        Number of epochs to wait before early stop
        if the metric stops getting improved
    filename : str or None
        Filename for storing the model checkpoint
    """
    def __init__(self, mode='higher', patience=10, filename=None):
        if filename is None:
            dt = datetime.datetime.now()
            filename = 'early_stop_{}_{:02d}-{:02d}-{:02d}.pth'.format(
                dt.date(), dt.hour, dt.minute, dt.second)

        assert mode in ['higher', 'lower']
        self.mode = mode
        if self.mode == 'higher':
            self._check = self._check_higher
        else:
            self._check = self._check_lower

        self.patience = patience
        self.counter = 0
        self.filename = filename
        self.best_score = None
        self.early_stop = False

    def _check_higher(self, score, prev_best_score):
        return (score > prev_best_score)

    def _check_lower(self, score, prev_best_score):
        return (score < prev_best_score)

    def step(self, score, model):
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(model)
        elif self._check(score, self.best_score):
            self.best_score = score
            self.save_checkpoint(model)
            self.counter = 0
        else:
            self.counter += 1
            print(
                f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        return self.early_stop

    def save_checkpoint(self, model):
        '''Saves model when the metric on the validation set gets improved.'''
        torch.save({'model_state_dict': model.state_dict()}, self.filename)

    def load_checkpoint(self, model):
        '''Load model saved with early stopping.'''
        model.load_state_dict(torch.load(self.filename)['model_state_dict'])

def collate_molgraphs_for_classification(data):
    """Batching a list of datapoints for dataloader in classification tasks.

    Parameters
    ----------
    data : list of 4-tuples
        Each tuple is for a single datapoint, consisting of
        a SMILES, a DGLGraph, all-task labels and all-task weights

    Returns
    -------
    smiles : list
        List of smiles
    bg : BatchedDGLGraph
        Batched DGLGraphs
    labels : Tensor of dtype float32 and shape (B, T)
        Batched datapoint labels. B is len(data) and
        T is the number of total tasks.
    weights : Tensor of dtype float32 and shape (B, T)
        Batched datapoint weights. T is the number of
        total tasks.
    """
    smiles, graphs, labels, mask = map(list, zip(*data))
    bg = dgl.batch(graphs)
    bg.set_n_initializer(dgl.init.zero_initializer)
    bg.set_e_initializer(dgl.init.zero_initializer)
    labels = torch.stack(labels, dim=0)
    mask = torch.stack(mask, dim=0)
    return smiles, bg, labels, mask

def collate_molgraphs_for_regression(data):
    """Batching a list of datapoints for dataloader in regression tasks.

    Parameters
    ----------
    data : list of 3-tuples
        Each tuple is for a single datapoint, consisting of
        a SMILES, a DGLGraph and all-task labels.

    Returns
    -------
    smiles : list
        List of smiles
    bg : BatchedDGLGraph
        Batched DGLGraphs
    labels : Tensor of dtype float32 and shape (B, T)
        Batched datapoint labels. B is len(data) and
        T is the number of total tasks.
    """
    smiles, graphs, labels = map(list, zip(*data))
    bg = dgl.batch(graphs)
    bg.set_n_initializer(dgl.init.zero_initializer)
    bg.set_e_initializer(dgl.init.zero_initializer)
    labels = torch.stack(labels, dim=0)
    return smiles, bg, labels

def load_dataset_for_regression(dataset_name):
    """Load dataset for regression tasks.

    Parameters
    ----------
    dataset_name : str
        Name of the dataset.

    Returns
    -------
    train_set
        Subset for training.
    val_set
        Subset for validation.
    test_set
        Subset for test.
    """
    assert dataset_name in ['Alchemy', 'ESOL']
    if dataset_name == 'Alchemy':
        from dgl.data.chem import TencentAlchemyDataset
        train_set = TencentAlchemyDataset(mode='dev')
        val_set = TencentAlchemyDataset(mode='valid')
        test_set = None
    elif dataset_name == 'ESOL':
        from dgl.data.chem import ESOL
        atom_featurizer = ConcatAtomFeaturizer(
            atom_descriptor_funcs=[
                partial(atomic_number_one_hot, encode_unknown=True),
                partial(atom_total_degree_one_hot, encode_unknown=True),
                partial(atom_formal_charge_one_hot, encode_unknown=True),
                partial(atom_chiral_tag_one_hot, encode_unknown=True),
                partial(atom_total_num_H_one_hot, encode_unknown=True),
                partial(atom_hybridization_one_hot, encode_unknown=True),
                atom_is_aromatic_one_hot,
                atom_mass
            ]
        )
        dataset = ESOL(atom_featurizer=atom_featurizer)
